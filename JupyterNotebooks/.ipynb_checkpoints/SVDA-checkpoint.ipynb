{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from operator import attrgetter\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from math import tan, pi\n",
    "import scipy.spatial as spatial\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction from ATL03 product (HDF5 formatted files)\n",
    "#Using only a single file for this example\n",
    "ATL03_input_path = '/raid-macon/ICESAT2/ATL03/ATL03_20200320133708_12950614_003_01.h5'\n",
    "#ATL03_20200320133708_12950614_003_01.h5\n",
    "#file is 4.7GB and is available at https://nsidc.org/data/ATL03\n",
    "\n",
    "# Output\n",
    "ATL03_output_path = '/home/bodo/Dropbox/soft/github/ICESat-2_SVDA/ATL03_example_data/hdf'\n",
    "\n",
    "#Region of interest to be clipped from ATL08 file:\n",
    "ROI_fname = '/home/bodo/Dropbox/soft/github/ICESat-2_SVDA/ATL08_example_data/ROI_westernNamibia.shp'\n",
    "\n",
    "EPSG_Code='epsg:32733'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py, glob, sys, warnings, tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "from pyproj import Transformer\n",
    "from pyproj import proj\n",
    "\n",
    "sys.path.append('/home/bodo/Dropbox/soft/github/ICESat-2_SVDA/python')\n",
    "\n",
    "from SVDA_helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal photons extraction from ATL03 data product\n",
    "\n",
    "Python code below allows to retrieve the desired data from ICESat-2 ATL03 data product, the data are then organized in a dataframe for each ground track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /raid-macon/ICESAT2/ATL03/ATL03_20200320133708_12950614_003_01.h5: gt1l, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bodo/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt1r, "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid location identifier (invalid location identifier)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6d72b142320d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s, '\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlat_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_ph_along\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_conf_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mlat_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mATL03\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mATL03_SDS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/lat_ph'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mlon_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mATL03\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mATL03_SDS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/lon_ph'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mh_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mATL03\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mATL03_SDS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/h_ph'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid location identifier (invalid location identifier)"
     ]
    }
   ],
   "source": [
    "ATL03_files = list(glob.glob(ATL03_input_path))\n",
    "for f in ATL03_files:\n",
    "    print('Opening %s: '%os.path.basename(f),end='')\n",
    "    ATL03 = h5py.File(f,'r')\n",
    "\n",
    "    gtr = [g for g in ATL03.keys() if g.startswith('gt')]\n",
    "\n",
    "    ATL03_objs = []\n",
    "    ATL03.visit(ATL03_objs.append)                                           \n",
    "    ATL03_SDS = [o for o in ATL03_objs if isinstance(ATL03[o], h5py.Dataset)]\n",
    "\n",
    "    # Retrieve datasets\n",
    "    for b in gtr:\n",
    "        print('%s, '%b,end='')\n",
    "        lat_ph, lon_ph, h_ph, dist_ph_along, signal_conf_ph, gtx = ([] for i in range(6))\n",
    "        [lat_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/lat_ph') and b in g][0]][()]]\n",
    "        [lon_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/lon_ph') and b in g][0]][()]]\n",
    "        [h_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/h_ph') and b in g][0]][()]]   \n",
    "        [dist_ph_along.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/dist_ph_along') and b in g][0]][()]]\n",
    "        [signal_conf_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/signal_conf_ph') and b in g][0]][()]]\n",
    "\n",
    "        ATL03_df = pd.DataFrame({'Latitude': lat_ph, 'Longitude': lon_ph, 'Along-track_Distance': dist_ph_along,\n",
    "                                 'Photon_Height': h_ph, 'Signal_Confidence':signal_conf_ph})\n",
    "\n",
    "        del lat_ph, lon_ph, h_ph, dist_ph_along, signal_conf_ph\n",
    "\n",
    "        ATL03_df.loc[:, 'Land'] = ATL03_df.Signal_Confidence.map(lambda x: x[0])\n",
    "        ATL03_df = ATL03_df.drop(columns=['Signal_Confidence'])\n",
    "\n",
    "        # Transform coordinates into utm\n",
    "        x, y = np.array(ATL03_df['Longitude']), np.array(ATL03_df['Latitude'])\n",
    "        transformer = Transformer.from_crs('epsg:4326', EPSG_Code, always_xy=True)\n",
    "        xx, yy = transformer.transform(x, y)\n",
    "\n",
    "        # Save the utm coordinates into the dataframe\n",
    "        ATL03_df['Easting'] = xx \n",
    "        ATL03_df['Northing'] = yy\n",
    "\n",
    "        ATL03_df, rotation_data = get_atl_alongtrack(ATL03_df)\n",
    "\n",
    "        ROI = gp.GeoDataFrame.from_file(ROI_fname, crs='EPSG:4326')\n",
    "        minLon, minLat, maxLon, maxLat = ROI.envelope[0].bounds\n",
    "\n",
    "        # Subset the dataframe into the study area bounds\n",
    "        ATL03_df = ATL03_df.where(ATL03_df['Latitude'] > minLat)\n",
    "        ATL03_df = ATL03_df.where(ATL03_df['Latitude'] < maxLat)\n",
    "        ATL03_df = ATL03_df.where(ATL03_df['Longitude'] > minLon)\n",
    "        ATL03_df = ATL03_df.where(ATL03_df['Longitude'] < maxLon)\n",
    "        ATL03_df = ATL03_df.dropna()\n",
    "\n",
    "        ATL03.close()\n",
    "\n",
    "        #ATL03_df.to_csv('{}_{}.csv'.format(ATL03_files[23:-3], gtr), header=True)\n",
    "        if not os.path.exists(ATL03_output_path):\n",
    "            os.mkdir(ATL03_output_path)\n",
    "        ATL03_df.to_hdf(os.path.join(ATL03_output_path,'Land_%s_%s.hdf'%(os.path.basename(f)[0:14],b)), \n",
    "                        key='%s_%s'%(os.path.basename(f)[0:14],b), complevel=7)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground and preliminary canopy photons classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/bodo/Dropbox/soft/github/ICESat-2_SVDA/ATL03_example_data/hdf/Land_ATL03_20200320_gt1l.hdf: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2309 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detrending topography and photon height filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2309/2309 [09:03<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "ATL03_land_files = glob.glob(os.path.join(ATL03_output_path, 'Land_*.hdf'))\n",
    "for f in ATL03_land_files:\n",
    "    print('Opening %s: '%os.path.basename(f),end='')\n",
    "    ATL03_df = pd.read_hdf(f, mode='r')\n",
    "        \n",
    "    # Bining in the along-track direction (bins of 30 m)\n",
    "    rows_labels = [f\"[{i}, {i+30}])\" for i in range(int(min(ATL03_df['alongtrack'])), int(max(ATL03_df['alongtrack'])), 30)]\n",
    "    rows_bins = pd.IntervalIndex.from_tuples([(i, i+30) for i in range(int(min(ATL03_df['alongtrack'])), int(max(ATL03_df['alongtrack'])), 30)], closed=\"left\")\n",
    "    rows_binned = pd.cut(ATL03_df['alongtrack'], rows_bins, labels=rows_labels, precision=2, include_lowest=True)\n",
    "    rows_binned.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "    rows_binned.drop_duplicates(keep='first',inplace=True)\n",
    "    rows_left = np.asarray(rows_binned.map(attrgetter('left')))\n",
    "    rows_right = np.asarray(rows_binned.map(attrgetter('right')))\n",
    "    rows_left = rows_left[~np.isnan(rows_left)]\n",
    "    rows_right = rows_right[~np.isnan(rows_right)]\n",
    "\n",
    "    ground_ph = []\n",
    "    canop = []\n",
    "    print('Detrending topography and photon height filtering.')\n",
    "    for i in tqdm.tqdm(range(len(rows_left))):\n",
    "        df = ATL03_df.where((ATL03_df['alongtrack']>= rows_left[i]) & (ATL03_df['alongtrack'] < rows_right[i]))\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Topography detrending and outliers filtering\n",
    "        df['detrend'] = signal.detrend(df['Photon_Height'])\n",
    "        df = df.where((df['detrend']> -(30*tan((pi/180)*30))) & (df['detrend'] < (30*tan((pi/180)*30))))\n",
    "        df = df.dropna()\n",
    "        df = df.drop(columns=['detrend'])\n",
    "\n",
    "        # Retrieving photons between the 25th and the 75th percentiles\n",
    "        if len(df)>5:\n",
    "            df_grd = df.where((df['Photon_Height'] <= np.percentile(df['Photon_Height'],75)) & (df['Photon_Height'] > np.percentile(df['Photon_Height'],25))) \n",
    "            df_grd = df_grd.dropna()\n",
    "\n",
    "            #Topography detrend and outliers filtering\n",
    "            df_grd['detrend'] = signal.detrend(df_grd['Photon_Height'])\n",
    "            df_grd = df_grd.where((df_grd['detrend']> -(tan((pi/180)*30))) & (df_grd['detrend'] < (tan((pi/180)*30))))\n",
    "            df_grd = df_grd.dropna()\n",
    "\n",
    "        # Photons with height above the maximum of each 25th-75th bin are retrieved as preliminary photons\n",
    "            if len(df_grd)>5:\n",
    "                df_canop = df.where(df['Photon_Height'] > max(df_grd['Photon_Height']))  \n",
    "                df_canop = df_canop.dropna()\n",
    "                canop.append(df_canop)\n",
    "                ground_ph.append(df_grd)\n",
    "\n",
    "    # Preliminary canopy photons\n",
    "    canop = [j for j in canop if len(j)>=0]\n",
    "    if len(canop)>0:\n",
    "        Canopy = pd.concat(canop, axis=0)\n",
    "\n",
    "    latitude, longitude, along,cross, med, north, east = ([] for i in range(7))\n",
    "\n",
    "    ground_ph = [j for j in ground_ph if len(j)!=0]\n",
    "\n",
    "    # Final ground photons are the medians of each bin placed in the center of each bin\n",
    "    for df in ground_ph:\n",
    "        al = (max(df['alongtrack']) + min(df['alongtrack']))/2\n",
    "        along.append(al)\n",
    "        es = (max(df['Easting']) + min(df['Easting']))/2\n",
    "        east.append(es)\n",
    "        nd = (max(df['Northing']) + min(df['Northing']))/2\n",
    "        north.append(nd)\n",
    "        cr = (max(df['crosstrack']) + min(df['crosstrack']))/2\n",
    "        cross.append(cr)\n",
    "        lat = (max(df['Latitude']) + min(df['Latitude']))/2\n",
    "        latitude.append(lat)\n",
    "        lon = (max(df['Longitude']) + min(df['Longitude']))/2         \n",
    "        longitude.append(lon)\n",
    "        m = np.median(df['Photon_Height'])\n",
    "        med.append(m)\n",
    "\n",
    "    # Ground photons dataframe\n",
    "    median_df = pd.DataFrame({'Latitude': latitude, 'Longitude': longitude, 'alongtrack': along, 'crosstrack': cross,\n",
    "                              'Easting': east, 'Northing': north, 'Photon_Height':med})\n",
    "\n",
    "    # Saving the final ground photons into hdf file\n",
    "    if len(median_df) >5:\n",
    "        median_df.to_hdf(os.path.join(ATL03_output_path,'Ground_%s'%'_'.join(os.path.basename(f).split('_')[1::])), \n",
    "                         key='Ground_%s'%'_'.join(os.path.basename(f).split('_')[1::])[:-4])\n",
    "\n",
    "        # calculate the canopy height by subtracting the ground height of each canopy photon by interpolating the final ground photons\n",
    "        if len(Canopy) > 5:\n",
    "            Canopy = Canopy.where((Canopy['alongtrack'] > min(median_df['alongtrack'])) & (Canopy['alongtrack'] < max(median_df['alongtrack'])))\n",
    "            Canopy = Canopy.dropna()\n",
    "            f_interp1d = interp1d(median_df['alongtrack'], median_df['Photon_Height'], kind='cubic')\n",
    "            Canopy['Canopy_Height'] = Canopy['Photon_Height'] - f_interp1d(Canopy['alongtrack'])\n",
    "\n",
    "            Canopy.to_hdf(os.path.join(ATL03_output_path,'Pre_Canopy_%s'%'_'.join(os.path.basename(f).split('_')[1::])),\n",
    "                          key='Pre_Canopy_%s'%'_'.join(os.path.basename(f).split('_')[1::])[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canopy photons classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/bodo/Dropbox/soft/github/ICESat-2_SVDA/ATL03_example_data/hdf/Pre_Canopy_ATL03_20200320_gt1l.hdf: "
     ]
    }
   ],
   "source": [
    "ATL03_precanopy_files = glob.glob(os.path.join(ATL03_output_path, 'Pre_Canopy_*.hdf'))\n",
    "for f in ATL03_precanopy_files:\n",
    "    print('Opening %s: '%os.path.basename(f),end='')\n",
    "    Canopy = pd.read_hdf(f, mode='r')\n",
    "\n",
    "    Canopy = Canopy.where(Canopy['Canopy_Height']>=3)\n",
    "    Canopy = Canopy.dropna()\n",
    "\n",
    "    # Easting, Northing and Canopy Height scaling\n",
    "    if len(Canopy)>5:\n",
    "        df = Canopy\n",
    "        df['z'] = (df['Canopy_Height']-np.min(df['Canopy_Height']))/(np.max(df['Canopy_Height'])-np.min(df['Canopy_Height']))\n",
    "        df['x'] = (df['alongtrack']-np.min(df['alongtrack']))/(np.max(df['alongtrack'])-np.min(df['alongtrack']))\n",
    "\n",
    "        X = np.array(df[['x', 'z']])\n",
    "        T = spatial.cKDTree(X)\n",
    "\n",
    "        index = []\n",
    "        for i in range(len(df)):\n",
    "            idx = T.query_ball_point(X[i], r = 0.01)\n",
    "            index.append(len(idx))\n",
    "        df['NN'] = index\n",
    "\n",
    "        # Filtering out canopy photons with number of neighbors below the 15th percentile of the number of neighbors\n",
    "        ddf = df.where(df['NN'] >=6)\n",
    "        ddf = ddf.dropna()\n",
    "\n",
    "        # Binning the along-track direction (bins of 10 m)\n",
    "        if len(ddf)>0:\n",
    "            rows_labels = [f\"[{i}, {i+10}])\" for i in range(int(min(ddf['alongtrack'])), int(max(ddf['alongtrack'])), 10)]\n",
    "            rows_bins = pd.IntervalIndex.from_tuples([(i, i+10) for i in range(int(min(ddf['alongtrack'])), \n",
    "                                                                               int(max(ddf['alongtrack'])), 10)], closed=\"left\")\n",
    "\n",
    "            rows_binned = pd.cut(ddf['alongtrack'], rows_bins, labels=rows_labels, precision=2, include_lowest=True)\n",
    "            rows_binned.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "            rows_binned.drop_duplicates(keep='first',inplace=True)\n",
    "            rows_left = np.asarray(rows_binned.map(attrgetter('left')))\n",
    "            rows_right = np.asarray(rows_binned.map(attrgetter('right')))\n",
    "            rows_left = rows_left[~np.isnan(rows_left)]\n",
    "            rows_right = rows_right[~np.isnan(rows_right)]\n",
    "\n",
    "            df_canop = []\n",
    "            print('Filtering along track')\n",
    "            for i in tqdm.tqdm(range(len(rows_left))):\n",
    "                dff = ddf.where((ddf['alongtrack']>= rows_left[i]) & (ddf['alongtrack'] < rows_right[i]))\n",
    "                dff = dff.dropna()\n",
    "                dff['Photons_Numb'] = len(dff)\n",
    "            # Calculate the maximum photon height of each bin\n",
    "                if len(dff)>0:\n",
    "                    dd = dff.where(dff['Canopy_Height']==max(dff['Canopy_Height']))\n",
    "                    dd = dd.dropna()\n",
    "                    df_canop.append(dd)\n",
    "            df_canop = [j for j in df_canop if len(j)>=0]\n",
    "            if len(df_canop)>0:\n",
    "                toc = pd.concat(df_canop, axis=0)\n",
    "                # Save the canopy and top of canopy photons into new hdf files \n",
    "                toc.to_hdf(os.path.join(ATL03_output_path,'TOC_%s'%'_'.join(os.path.basename(f).split('_')[1::])),\n",
    "                          key='TOC_%s'%'_'.join(os.path.basename(f).split('_')[1::])[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grass photons classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bodo/miniconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Pre_Canopy_ATL03_20200320_gt1l.hdf: "
     ]
    }
   ],
   "source": [
    "ATL03_precanopy_files = glob.glob(os.path.join(ATL03_output_path, 'Pre_Canopy_*.hdf'))\n",
    "for f in ATL03_precanopy_files:\n",
    "    print('Opening %s: '%os.path.basename(f),end='')\n",
    "    Canopy = pd.read_hdf(f, mode='r')\n",
    "\n",
    "    # All photons with canopy height between 0.5 m and 3 m\n",
    "    Canopy = Canopy.where((Canopy['Canopy_Height']>=0.5) & (Canopy['Canopy_Height']<3))\n",
    "    Canopy = Canopy.dropna()\n",
    "\n",
    "    # Easting, Northing and Canopy Height scaling\n",
    "    if len(Canopy)>5:\n",
    "        df = Canopy\n",
    "        df['z'] = (df['Canopy_Height']-np.min(df['Canopy_Height']))/(np.max(df['Canopy_Height'])-np.min(df['Canopy_Height']))\n",
    "        df['x'] = (df['alongtrack']-np.min(df['alongtrack']))/(np.max(df['alongtrack'])-np.min(df['alongtrack']))\n",
    "\n",
    "        X = np.array(df[['x', 'z']])\n",
    "        T = spatial.cKDTree(X)\n",
    "\n",
    "        index = []\n",
    "        for i in range(len(df)):\n",
    "            idx = T.query_ball_point(X[i], r = 0.01)\n",
    "            index.append(len(idx))\n",
    "        df['NN'] = index\n",
    "\n",
    "        # Filtering out canopy photons with number of neighbors below the 15th percentile of the number of neighbors\n",
    "        ddf = df.where(df['NN'] >=6)\n",
    "        ddf = ddf.dropna()\n",
    "\n",
    "        # Binning the along-track direction (bins of 10 m)\n",
    "        if len(ddf)>0:\n",
    "            rows_labels = [f\"[{i}, {i+10}])\" for i in range(int(min(ddf['alongtrack'])), int(max(ddf['alongtrack'])), 10)]\n",
    "            rows_bins = pd.IntervalIndex.from_tuples([(i, i+10) for i in range(int(min(ddf['alongtrack'])), \n",
    "                                                                               int(max(ddf['alongtrack'])), 10)], closed=\"left\")\n",
    "\n",
    "            rows_binned = pd.cut(ddf['alongtrack'], rows_bins, labels=rows_labels, precision=2, include_lowest=True)\n",
    "            rows_binned.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "            rows_binned.drop_duplicates(keep='first',inplace=True)\n",
    "            rows_left = np.asarray(rows_binned.map(attrgetter('left')))\n",
    "            rows_right = np.asarray(rows_binned.map(attrgetter('right')))\n",
    "            rows_left = rows_left[~np.isnan(rows_left)]\n",
    "            rows_right = rows_right[~np.isnan(rows_right)]\n",
    "\n",
    "            df_canop = []\n",
    "            for i in range(len(rows_left)):\n",
    "                dff = ddf.where((ddf['alongtrack']>= rows_left[i]) & (ddf['alongtrack'] < rows_right[i]))\n",
    "                dff = dff.dropna()\n",
    "            # Calculate the maximum photon height of each bin\n",
    "                if len(dff)>0:\n",
    "                    dd = dff.where(dff['Canopy_Height']==max(dff['Canopy_Height']))\n",
    "                    dd = dd.dropna()\n",
    "                    df_canop.append(dd)\n",
    "            df_canop = [j for j in df_canop if len(j)>=0]\n",
    "            if len(df_canop)>0:\n",
    "                toc = pd.concat(df_canop, axis=0)\n",
    "                # Save the canopy and top of canopy photons into new csv files \n",
    "                toc.to_hdf(os.path.join(ATL03_output_path,'Grass_height_%s'%'_'.join(os.path.basename(f).split('_')[1::])),\n",
    "                          key='Grass_height_%s'%'_'.join(os.path.basename(f).split('_')[1::])[:-4])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesat2",
   "language": "python",
   "name": "icesat2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

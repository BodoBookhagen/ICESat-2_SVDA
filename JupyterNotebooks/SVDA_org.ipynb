{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from operator import attrgetter\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from math import tan, pi\n",
    "import scipy.spatial as spatial\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal photons extraction from ATL03 data product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python code below allows to retrieve the desired data from ICESat-2 ATL03 data product,\n",
    "   the data are then organized in a dataframe for each ground track\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "               \n",
    "ATL03_files = ('/data/atmani/ATL03_new/')\n",
    "\n",
    "for f in os.listdir(ATL08_files):\n",
    "    if f.startswith('ATL03') and f.endswith('.h5'):\n",
    "        fid = os.path.join(ATL03_files, f)\n",
    "        ATL03 = h5py.File(fid,'r')\n",
    "        \n",
    "        gtr = [g for g in ATL03.keys() if g.startswith('gt')]\n",
    "        \n",
    "        ATL03_objs = []\n",
    "        ATL03.visit(ATL03_objs.append)                                           \n",
    "        ATL03_SDS = [o for o in ATL03_objs if isinstance(ATL03[o], h5py.Dataset)]\n",
    "        \n",
    "        lat_ph, lon_ph, h_ph, dist_ph_along, signal_conf_ph, gtx = ([] for i in range(6))\n",
    "        \n",
    "        # Retrieve datasets\n",
    "        for b in gtr:\n",
    "            #for c in ATL03:\n",
    "            [lat_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/lat_ph') and b in g][0]][()]]\n",
    "            [lon_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/lon_ph') and b in g][0]][()]]\n",
    "            [h_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/h_ph') and b in g][0]][()]]   \n",
    "            [dist_ph_along.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/dist_ph_along') and b in g][0]][()]]\n",
    "            [signal_conf_ph.append(h) for h in ATL03[[g for g in ATL03_SDS if g.endswith('/signal_conf_ph') and b in g][0]][()]]\n",
    "            \n",
    "            ATL03_df = pd.DataFrame({'Latitude': lat_ph, 'Longitude': lon_ph, 'Along-track_Distance': dist_ph_along,\n",
    "                                     'Photon_Height': h_ph, 'Signal_Confidence':signal_conf_ph})\n",
    "            \n",
    "            del lat_ph, lon_ph, h_ph, dist_ph_along, signal_conf_ph\n",
    "            \n",
    "            ATL03_df.loc[:, 'Land'] = ATL03_df.Signal_Confidence.map(lambda x: x[0])\n",
    "            ATL03_df = ATL03_df.drop(columns=['Signal_Confidence'])\n",
    "            \n",
    "            # Transform coordinates into utm\n",
    "            from pyproj import proj\n",
    "            from pyproj import Transformer\n",
    "            x, y = np.array(ATL03_df['Longitude']), np.array(ATL03_df['Latitude'])\n",
    "            transformer = Transformer.from_crs('epsg:4326', 'epsg:32733', always_xy=True)\n",
    "            xx, yy = transformer.transform(x, y)\n",
    "            \n",
    "            # Save the utm coordinates into the dataframe\n",
    "            ATL03_df['Easting'] = xx \n",
    "            ATL03_df['Northing'] = yy\n",
    "            \n",
    "            ATL03_df, rotation_data = get_atl_alongtrack(ATL03_df)\n",
    "\n",
    "            ROI = gpd.GeoDataFrame.from_file('/data/atmani/study_area/Msc_Study_area.shp', crs='EPSG:4326') \n",
    "            \n",
    "            minLon, minLat, maxLon, maxLat = ROI.envelope[0].bounds\n",
    "            \n",
    "            # Subset the dataframe into the study area bounds\n",
    "            ATL03_df = ATL03_df.where(ATL03_df['Latitude'] > minLat)\n",
    "            ATL03_df = ATL03_df.where(ATL03_df['Latitude'] < maxLat)\n",
    "            ATL03_df = ATL03_df.where(ATL03_df['Longitude'] > minLon)\n",
    "            ATL03_df = ATL03_df.where(ATL03_df['Longitude'] < maxLon)\n",
    "            ATL03_df = ATL03_df.dropna()\n",
    "            \n",
    "            ATL03_df.to_csv('{}_{}.csv'.format(ATL03_files[23:-3], gtr), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground and preliminary canopy photons classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/atmani/Sig_Ph_06062021/sub/\"\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    if f.startswith('Fi') and f.endswith('.csv'):\n",
    "        fid = os.path.join(path, f)\n",
    "        ATL03_df = pd.read_csv(fid)\n",
    "        \n",
    "        # Bining in the along-track direction (bins of 30 m)\n",
    "        rows_labels = [f\"[{i}, {i+30}])\" for i in range(int(min(ATL03_df['alongtrack'])), int(max(ATL03_df['alongtrack'])), 30)]\n",
    "        rows_bins = pd.IntervalIndex.from_tuples([(i, i+30) for i in range(int(min(ATL03_df['alongtrack'])), int(max(ATL03_df['alongtrack'])), 30)], closed=\"left\")\n",
    "        rows_binned = pd.cut(ATL03_df['alongtrack'], rows_bins, labels=rows_labels, precision=2, include_lowest=True)\n",
    "        rows_binned.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "        rows_binned.drop_duplicates(keep='first',inplace=True)\n",
    "        rows_left = np.asarray(rows_binned.map(attrgetter('left')))\n",
    "        rows_right = np.asarray(rows_binned.map(attrgetter('right')))\n",
    "        rows_left = rows_left[~np.isnan(rows_left)]\n",
    "        rows_right = rows_right[~np.isnan(rows_right)]\n",
    "\n",
    "        ground_ph = []\n",
    "        canop = []\n",
    "        for i in range(len(rows_left)):\n",
    "            df = ATL03_df.where((ATL03_df['alongtrack']>= rows_left[i]) & (ATL03_df['alongtrack'] < rows_right[i]))\n",
    "            df = df.dropna()\n",
    "\n",
    "            # Topography detrending and outliers filtering\n",
    "            df['detrend'] = signal.detrend(df['Photon_Height'])\n",
    "            df = df.where((df['detrend']> -(30*tan((pi/180)*30))) & (df['detrend'] < (30*tan((pi/180)*30))))\n",
    "            df = df.dropna()\n",
    "            df = df.drop(columns=['detrend'])\n",
    "            \n",
    "            # Retrieving photons between the 25th and the 75th percentiles\n",
    "            if len(df)>5:\n",
    "                df_grd = df.where((df['Photon_Height'] <= np.percentile(df['Photon_Height'],75)) & (df['Photon_Height'] > np.percentile(df['Photon_Height'],25))) \n",
    "                df_grd = df_grd.dropna()\n",
    "                \n",
    "                #Topography detrend and outliers filtering\n",
    "                df_grd['detrend'] = signal.detrend(df_grd['Photon_Height'])\n",
    "                df_grd = df_grd.where((df_grd['detrend']> -(tan((pi/180)*30))) & (df_grd['detrend'] < (tan((pi/180)*30))))\n",
    "                df_grd = df_grd.dropna()\n",
    "\n",
    "            # Photons with height above the maximum of each 25th-75th bin are retrieved as preliminary photons\n",
    "                if len(df_grd)>5:\n",
    "                    df_canop = df.where(df['Photon_Height'] > max(df_grd['Photon_Height']))  \n",
    "                    df_canop = df_canop.dropna()\n",
    "                    canop.append(df_canop)\n",
    "                    ground_ph.append(df_grd)\n",
    "\n",
    "        # Preliminary canopy photons\n",
    "        canop = [j for j in canop if len(j)>=0]\n",
    "        if len(canop)>0:\n",
    "            Canopy = pd.concat(canop, axis=0)\n",
    "\n",
    "        latitude, longitude, along,cross, med, north, east = ([] for i in range(7))\n",
    "\n",
    "        ground_ph = [j for j in ground_ph if len(j)!=0]\n",
    "\n",
    "        # Final ground photons are the medians of each bin placed in the center of each bin\n",
    "        for df in ground_ph:\n",
    "            al = (max(df['alongtrack']) + min(df['alongtrack']))/2\n",
    "            along.append(al)\n",
    "            es = (max(df['Easting']) + min(df['Easting']))/2\n",
    "            east.append(es)\n",
    "            nd = (max(df['Northing']) + min(df['Northing']))/2\n",
    "            north.append(nd)\n",
    "            cr = (max(df['crosstrack']) + min(df['crosstrack']))/2\n",
    "            cross.append(cr)\n",
    "            lat = (max(df['Latitude']) + min(df['Latitude']))/2\n",
    "            latitude.append(lat)\n",
    "            lon = (max(df['Longitude']) + min(df['Longitude']))/2         \n",
    "            longitude.append(lon)\n",
    "            m = np.median(df['Photon_Height'])\n",
    "            med.append(m)\n",
    "            \n",
    "        # Ground photons dataframe\n",
    "        median_df = pd.DataFrame({'Latitude': latitude, 'Longitude': longitude, 'alongtrack': along, 'crosstrack': cross,\n",
    "                                  'Easting': east, 'Northing': north, 'Photon_Height':med})\n",
    "        \n",
    "        # Saving the final ground photons into csv file\n",
    "        if len(median_df) >5:\n",
    "            pathh = \"/data/atmani/New_Gr_CH_Classifi_17_09_2021/27_09_2021/\"\n",
    "            median_df.to_csv(os.path.join(pathh,r'Ground_{}.csv'.format(fid[36:-4])), header=True)\n",
    "    \n",
    "            # calculate the canopy height by subtracting the ground height of each canopy photon by interpolating the final ground photons\n",
    "            if len(Canopy) > 5:\n",
    "                Canopy = Canopy.where((Canopy['alongtrack'] > min(median_df['alongtrack'])) & (Canopy['alongtrack'] < max(median_df['alongtrack'])))\n",
    "                Canopy = Canopy.dropna()\n",
    "\n",
    "            if len(Canopy) > 5:\n",
    "                f = interp1d(median_df['alongtrack'], median_df['Photon_Height'], kind='cubic')\n",
    "\n",
    "                Canopy['Canopy_Height'] = Canopy['Photon_Height'] - f(Canopy['alongtrack'])\n",
    "                \n",
    "                Canopy.to_csv(os.path.join(pathh,r'Pre_Canopy_{}.csv'.format(fid[36:-4])), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canopy photons classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/atmani/TOC_03_10_2021/'\n",
    "\n",
    "gt = []\n",
    "for f in os.listdir(path):\n",
    "    if f.startswith('Pre') and f.endswith('.csv'):\n",
    "        fid = os.path.join(path, f)\n",
    "        Canopy = pd.read_csv(fid)\n",
    "\n",
    "        Canopy = Canopy.where(Canopy['Canopy_Height']>=3)\n",
    "        Canopy = Canopy.dropna()\n",
    "\n",
    "        # Easting, Northing and Canopy Height scaling\n",
    "        if len(Canopy)>5:\n",
    "            df = Canopy\n",
    "            df['z'] = (df['Canopy_Height']-np.min(df['Canopy_Height']))/(np.max(df['Canopy_Height'])-np.min(df['Canopy_Height']))\n",
    "            df['x'] = (df['alongtrack']-np.min(df['alongtrack']))/(np.max(df['alongtrack'])-np.min(df['alongtrack']))\n",
    "\n",
    "            X = np.array(df[['x', 'z']])\n",
    "             T = spatial.cKDTree(X)\n",
    "\n",
    "            index = []\n",
    "            for i in range(len(df)):\n",
    "                idx = T.query_ball_point(X[i], r = 0.01)\n",
    "                index.append(len(idx))\n",
    "            df['NN'] = index\n",
    "\n",
    "            # Filtering out canopy photons with number of neighbors below the 15th percentile of the number of neighbors\n",
    "            ddf = df.where(df['NN'] >=6)\n",
    "            ddf = ddf.dropna()\n",
    "\n",
    "            # Binning the along-track direction (bins of 10 m)\n",
    "            if len(ddf)>0:\n",
    "                rows_labels = [f\"[{i}, {i+10}])\" for i in range(int(min(ddf['alongtrack'])), int(max(ddf['alongtrack'])), 10)]\n",
    "                rows_bins = pd.IntervalIndex.from_tuples([(i, i+10) for i in range(int(min(ddf['alongtrack'])), \n",
    "                                                                                   int(max(ddf['alongtrack'])), 10)], closed=\"left\")\n",
    "\n",
    "                rows_binned = pd.cut(ddf['alongtrack'], rows_bins, labels=rows_labels, precision=2, include_lowest=True)\n",
    "                rows_binned.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "                rows_binned.drop_duplicates(keep='first',inplace=True)\n",
    "                rows_left = np.asarray(rows_binned.map(attrgetter('left')))\n",
    "                rows_right = np.asarray(rows_binned.map(attrgetter('right')))\n",
    "                rows_left = rows_left[~np.isnan(rows_left)]\n",
    "                rows_right = rows_right[~np.isnan(rows_right)]\n",
    "\n",
    "                df_canop = []\n",
    "                for i in range(len(rows_left)):\n",
    "                    dff = ddf.where((ddf['alongtrack']>= rows_left[i]) & (ddf['alongtrack'] < rows_right[i]))\n",
    "                    dff = dff.dropna()\n",
    "                    dff['Photons_Numb'] = len(dff)\n",
    "                # Calculate the maximum photon height of each bin\n",
    "                    if len(dff)>0:\n",
    "                        dd = dff.where(dff['Canopy_Height']==max(dff['Canopy_Height']))\n",
    "                        dd = dd.dropna()\n",
    "                        df_canop.append(dd)\n",
    "                df_canop = [j for j in df_canop if len(j)>=0]\n",
    "                if len(df_canop)>0:\n",
    "\n",
    "                    toc = pd.concat(df_canop, axis=0)\n",
    "\n",
    "                    # Save the canopy and top of canopy photons into new csv files \n",
    "                    pathh = '/data/atmani/TOC_03_10_2021/TOC_02_01_2022_6pts_0.1/'\n",
    "                    toc.to_csv(os.path.join(pathh,r'TOC_{}.csv'.format(toc['gt'].iloc[0])), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grass photons classificaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/atmani/TOC_03_10_2021/'\n",
    "\n",
    "gt = []\n",
    "for f in os.listdir(path):\n",
    "    if f.startswith('Pre') and f.endswith('.csv'):\n",
    "        fid = os.path.join(path, f)\n",
    "        Canopy = pd.read_csv(fid)\n",
    "        \n",
    "        # All photons with canopy height between 0.5 m and 3 m\n",
    "        Canopy = Canopy.where((Canopy['Canopy_Height']>=0.5) & (Canopy['Canopy_Height']<3))\n",
    "        Canopy = Canopy.dropna()\n",
    "\n",
    "        # Easting, Northing and Canopy Height scaling\n",
    "        if len(Canopy)>5:\n",
    "            df = Canopy\n",
    "            df['z'] = (df['Canopy_Height']-np.min(df['Canopy_Height']))/(np.max(df['Canopy_Height'])-np.min(df['Canopy_Height']))\n",
    "            df['x'] = (df['alongtrack']-np.min(df['alongtrack']))/(np.max(df['alongtrack'])-np.min(df['alongtrack']))\n",
    "\n",
    "            X = np.array(df[['x', 'z']])\n",
    "            T = spatial.cKDTree(X)\n",
    "\n",
    "            index = []\n",
    "            for i in range(len(df)):\n",
    "                idx = T.query_ball_point(X[i], r = 0.01)\n",
    "                index.append(len(idx))\n",
    "            df['NN'] = index\n",
    "\n",
    "            # Filtering out canopy photons with number of neighbors below the 15th percentile of the number of neighbors\n",
    "            ddf = df.where(df['NN'] >=6)\n",
    "            ddf = ddf.dropna()\n",
    "\n",
    "            # Binning the along-track direction (bins of 10 m)\n",
    "            if len(ddf)>0:\n",
    "                rows_labels = [f\"[{i}, {i+10}])\" for i in range(int(min(ddf['alongtrack'])), int(max(ddf['alongtrack'])), 10)]\n",
    "                rows_bins = pd.IntervalIndex.from_tuples([(i, i+10) for i in range(int(min(ddf['alongtrack'])), \n",
    "                                                                                   int(max(ddf['alongtrack'])), 10)], closed=\"left\")\n",
    "\n",
    "                rows_binned = pd.cut(ddf['alongtrack'], rows_bins, labels=rows_labels, precision=2, include_lowest=True)\n",
    "                rows_binned.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "                rows_binned.drop_duplicates(keep='first',inplace=True)\n",
    "                rows_left = np.asarray(rows_binned.map(attrgetter('left')))\n",
    "                rows_right = np.asarray(rows_binned.map(attrgetter('right')))\n",
    "                rows_left = rows_left[~np.isnan(rows_left)]\n",
    "                rows_right = rows_right[~np.isnan(rows_right)]\n",
    "\n",
    "                df_canop = []\n",
    "                for i in range(len(rows_left)):\n",
    "                    dff = ddf.where((ddf['alongtrack']>= rows_left[i]) & (ddf['alongtrack'] < rows_right[i]))\n",
    "                    dff = dff.dropna()\n",
    "                # Calculate the maximum photon height of each bin\n",
    "                    if len(dff)>0:\n",
    "                        dd = dff.where(dff['Canopy_Height']==max(dff['Canopy_Height']))\n",
    "                        dd = dd.dropna()\n",
    "                        df_canop.append(dd)\n",
    "                df_canop = [j for j in df_canop if len(j)>=0]\n",
    "                if len(df_canop)>0:\n",
    "\n",
    "                    toc = pd.concat(df_canop, axis=0)\n",
    "\n",
    "                    # Save the canopy and top of canopy photons into new csv files \n",
    "                    pathh = '/data/atmani/TOC_03_10_2021/Grass_02_01_2022_6pts_0.1/'\n",
    "                    toc.to_csv(os.path.join(pathh,r'Grass_height_{}.csv'.format(toc['gt'].iloc[0])), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Along-track distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The functions below are used to calculate the along-track distance, the functions are the same used by\n",
    "    PhoREAL (Photon Research and Engineering Analysis Library) https://github.com/icesat-2UT/PhoREAL\"\"\"\n",
    "\n",
    "\n",
    "def getCoordRotFwd(xIn,yIn,R_mat,xRotPt,yRotPt,desiredAngle):\n",
    "   \n",
    "    # Get shape of input X,Y data\n",
    "    xInShape = np.shape(xIn)\n",
    "    yInShape = np.shape(yIn)\n",
    "    \n",
    "    # If shape of arrays are (N,1), then make them (N,)\n",
    "    xIn = xIn.ravel()\n",
    "    yIn = yIn.ravel()\n",
    "    \n",
    "    # Suppress warnings that may come from np.polyfit\n",
    "    if not sys.warnoptions:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    # endif\n",
    "    \n",
    "    # If Rmatrix, xRotPt, and yRotPt are empty, then compute them\n",
    "    if(len(R_mat)==0 and len(xRotPt)==0 and len(yRotPt)==0):\n",
    "        \n",
    "        # Get current angle of linear fit data\n",
    "        x1 = xIn[0]\n",
    "        x2 = xIn[-1]\n",
    "        y1 = yIn[0]\n",
    "        y2 = yIn[-1]\n",
    "        # endif\n",
    "        deltaX = x2 - x1\n",
    "        deltaY = y2 - y1\n",
    "        theta = np.arctan2(deltaY,deltaX)\n",
    "        \n",
    "        # Get angle to rotate through\n",
    "        phi = np.radians(desiredAngle) - theta\n",
    "        \n",
    "        # Get rotation matrix\n",
    "        R_mat = np.matrix(np.array([[np.cos(phi), -np.sin(phi)],[np.sin(phi), np.cos(phi)]]))\n",
    "        \n",
    "        # Get X,Y rotation points\n",
    "        xRotPt = x1\n",
    "        yRotPt = y1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Get angle to rotate through\n",
    "        phi = np.arccos(R_mat[0,0])\n",
    "    \n",
    "    # endif\n",
    "    \n",
    "    # Translate data to X,Y rotation point\n",
    "    xTranslated = xIn - xRotPt\n",
    "    yTranslated = yIn - yRotPt\n",
    "    \n",
    "    # Convert np array to np matrix\n",
    "    xTranslated_mat = np.matrix(xTranslated)\n",
    "    yTranslated_mat = np.matrix(yTranslated)\n",
    "    \n",
    "    # Get shape of np X,Y matrices\n",
    "    (xTranslated_matRows,xTranslated_matCols) = xTranslated_mat.shape\n",
    "    (yTranslated_matRows,yTranslated_matCols) = yTranslated_mat.shape\n",
    "    \n",
    "    # Make X input a row vector\n",
    "    if(xTranslated_matRows > 1):\n",
    "        xTranslated_mat = np.transpose(xTranslated_mat)\n",
    "    #endif\n",
    "    \n",
    "    # Make Y input a row vector\n",
    "    if(yTranslated_matRows > 1):\n",
    "        yTranslated_mat = np.transpose(yTranslated_mat)\n",
    "    #endif\n",
    "    \n",
    "    # Put X,Y data into separate rows of matrix\n",
    "    xyTranslated_mat = np.concatenate((xTranslated_mat,yTranslated_mat))\n",
    "    \n",
    "    # Compute matrix multiplication to get rotated frame\n",
    "    measRot_mat = np.matmul(R_mat,xyTranslated_mat)\n",
    "                            \n",
    "    # Pull out X,Y rotated data\n",
    "    xRot_mat = measRot_mat[0,:]\n",
    "    yRot_mat = measRot_mat[1,:]\n",
    "    \n",
    "    # Convert X,Y matrices back to np arrays for output\n",
    "    xRot = np.array(xRot_mat)\n",
    "    yRot = np.array(yRot_mat)\n",
    "    \n",
    "    # Make X,Y rotated output the same shape as X,Y input\n",
    "    xRot = np.reshape(xRot,xInShape)\n",
    "    yRot = np.reshape(yRot,yInShape)\n",
    "    \n",
    "    # Reset warnings \n",
    "    warnings.resetwarnings()\n",
    "                   \n",
    "    # Return outputs\n",
    "    return xRot, yRot, R_mat, xRotPt, yRotPt, phi\n",
    "\n",
    "class AtlRotationStruct:\n",
    "    \n",
    "    # Define class with designated fields\n",
    "    def __init__(self, R_mat, xRotPt, yRotPt, desiredAngle, phi):\n",
    "        \n",
    "        self.R_mat = R_mat\n",
    "        self.xRotPt = xRotPt\n",
    "        self.yRotPt = yRotPt\n",
    "        self.desiredAngle = desiredAngle\n",
    "        self.phi = phi\n",
    "        \n",
    "# Function to calculate the along-track distance\n",
    "def get_atl_alongtrack(df):\n",
    "    easting = np.array(df['Easting'])\n",
    "    northing = np.array(df['Northing'])\n",
    "    \n",
    "    desiredAngle = 90\n",
    "    crossTrack, alongTrack, R_mat, xRotPt, yRotPt, phi = \\\n",
    "    getCoordRotFwd(easting, northing, [], [], [], desiredAngle)\n",
    "\n",
    "    df = pd.concat([df,pd.DataFrame(crossTrack, columns=['crosstrack'])],axis=1)\n",
    "    df = pd.concat([df,pd.DataFrame(alongTrack, columns=['alongtrack'])],axis=1)  \n",
    "\n",
    "    rotation_data = AtlRotationStruct(R_mat, xRotPt, yRotPt, desiredAngle, phi)\n",
    "    \n",
    "    return df, rotation_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
